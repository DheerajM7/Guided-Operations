{"ast":null,"code":"var _s = $RefreshSig$();\nimport { useState, useEffect } from 'react';\nconst useSpeechRecognition = () => {\n  _s();\n  const [isListening, setIsListening] = useState(false); // Mic listening state\n  const [transcript, setTranscript] = useState(''); // The transcribed text\n  const [recognition, setRecognition] = useState(null); // The SpeechRecognition instance\n\n  useEffect(() => {\n    // Check if the browser supports the SpeechRecognition API\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!SpeechRecognition) {\n      console.error('SpeechRecognition is not supported in this browser.');\n      return;\n    }\n    const recognitionInstance = new SpeechRecognition();\n    recognitionInstance.continuous = false;\n    recognitionInstance.interimResults = false;\n    recognitionInstance.lang = 'en-US';\n    recognitionInstance.onresult = event => {\n      const speechToText = event.results[0][0].transcript;\n      setTranscript(speechToText);\n      setIsListening(false); // Stop listening after the speech is transcribed\n    };\n    recognitionInstance.onend = () => {\n      setIsListening(false); // Reset listening state when recognition ends\n    };\n    setRecognition(recognitionInstance); // Store the instance for later use\n  }, []);\n  const startListening = () => {\n    if (recognition) {\n      recognition.start();\n      setIsListening(true);\n    } else {\n      console.error('SpeechRecognition is not initialized.');\n    }\n  };\n  const stopListening = () => {\n    if (recognition) {\n      recognition.stop();\n      setIsListening(false);\n    } else {\n      console.error('SpeechRecognition is not initialized.');\n    }\n  };\n  return {\n    isListening,\n    transcript,\n    startListening,\n    stopListening\n  };\n};\n_s(useSpeechRecognition, \"QwHeDc7X8SG4TAdZwfUVU/9t+Mw=\");\nexport default useSpeechRecognition;","map":{"version":3,"names":["useState","useEffect","useSpeechRecognition","_s","isListening","setIsListening","transcript","setTranscript","recognition","setRecognition","SpeechRecognition","window","webkitSpeechRecognition","console","error","recognitionInstance","continuous","interimResults","lang","onresult","event","speechToText","results","onend","startListening","start","stopListening","stop"],"sources":["/Users/dheerajmendu/Desktop/Milvian Group/guided-operations-main/frontend/src/components/useSpeechRecognition.js"],"sourcesContent":["import { useState, useEffect } from 'react';\n\nconst useSpeechRecognition = () => {\n  const [isListening, setIsListening] = useState(false); // Mic listening state\n  const [transcript, setTranscript] = useState(''); // The transcribed text\n  const [recognition, setRecognition] = useState(null); // The SpeechRecognition instance\n\n  useEffect(() => {\n    // Check if the browser supports the SpeechRecognition API\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (!SpeechRecognition) {\n      console.error('SpeechRecognition is not supported in this browser.');\n      return;\n    }\n\n    const recognitionInstance = new SpeechRecognition();\n    recognitionInstance.continuous = false;\n    recognitionInstance.interimResults = false;\n    recognitionInstance.lang = 'en-US';\n\n    recognitionInstance.onresult = (event) => {\n      const speechToText = event.results[0][0].transcript;\n      setTranscript(speechToText);\n      setIsListening(false); // Stop listening after the speech is transcribed\n    };\n\n    recognitionInstance.onend = () => {\n      setIsListening(false); // Reset listening state when recognition ends\n    };\n\n    setRecognition(recognitionInstance); // Store the instance for later use\n  }, []);\n\n  const startListening = () => {\n    if (recognition) {\n      recognition.start();\n      setIsListening(true);\n    } else {\n      console.error('SpeechRecognition is not initialized.');\n    }\n  };\n\n  const stopListening = () => {\n    if (recognition) {\n      recognition.stop();\n      setIsListening(false);\n    } else {\n      console.error('SpeechRecognition is not initialized.');\n    }\n  };\n\n  return { isListening, transcript, startListening, stopListening };\n};\n\nexport default useSpeechRecognition;\n"],"mappings":";AAAA,SAASA,QAAQ,EAAEC,SAAS,QAAQ,OAAO;AAE3C,MAAMC,oBAAoB,GAAGA,CAAA,KAAM;EAAAC,EAAA;EACjC,MAAM,CAACC,WAAW,EAAEC,cAAc,CAAC,GAAGL,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC;EACvD,MAAM,CAACM,UAAU,EAAEC,aAAa,CAAC,GAAGP,QAAQ,CAAC,EAAE,CAAC,CAAC,CAAC;EAClD,MAAM,CAACQ,WAAW,EAAEC,cAAc,CAAC,GAAGT,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;EAEtDC,SAAS,CAAC,MAAM;IACd;IACA,MAAMS,iBAAiB,GAAGC,MAAM,CAACD,iBAAiB,IAAIC,MAAM,CAACC,uBAAuB;IACpF,IAAI,CAACF,iBAAiB,EAAE;MACtBG,OAAO,CAACC,KAAK,CAAC,qDAAqD,CAAC;MACpE;IACF;IAEA,MAAMC,mBAAmB,GAAG,IAAIL,iBAAiB,CAAC,CAAC;IACnDK,mBAAmB,CAACC,UAAU,GAAG,KAAK;IACtCD,mBAAmB,CAACE,cAAc,GAAG,KAAK;IAC1CF,mBAAmB,CAACG,IAAI,GAAG,OAAO;IAElCH,mBAAmB,CAACI,QAAQ,GAAIC,KAAK,IAAK;MACxC,MAAMC,YAAY,GAAGD,KAAK,CAACE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAChB,UAAU;MACnDC,aAAa,CAACc,YAAY,CAAC;MAC3BhB,cAAc,CAAC,KAAK,CAAC,CAAC,CAAC;IACzB,CAAC;IAEDU,mBAAmB,CAACQ,KAAK,GAAG,MAAM;MAChClB,cAAc,CAAC,KAAK,CAAC,CAAC,CAAC;IACzB,CAAC;IAEDI,cAAc,CAACM,mBAAmB,CAAC,CAAC,CAAC;EACvC,CAAC,EAAE,EAAE,CAAC;EAEN,MAAMS,cAAc,GAAGA,CAAA,KAAM;IAC3B,IAAIhB,WAAW,EAAE;MACfA,WAAW,CAACiB,KAAK,CAAC,CAAC;MACnBpB,cAAc,CAAC,IAAI,CAAC;IACtB,CAAC,MAAM;MACLQ,OAAO,CAACC,KAAK,CAAC,uCAAuC,CAAC;IACxD;EACF,CAAC;EAED,MAAMY,aAAa,GAAGA,CAAA,KAAM;IAC1B,IAAIlB,WAAW,EAAE;MACfA,WAAW,CAACmB,IAAI,CAAC,CAAC;MAClBtB,cAAc,CAAC,KAAK,CAAC;IACvB,CAAC,MAAM;MACLQ,OAAO,CAACC,KAAK,CAAC,uCAAuC,CAAC;IACxD;EACF,CAAC;EAED,OAAO;IAAEV,WAAW;IAAEE,UAAU;IAAEkB,cAAc;IAAEE;EAAc,CAAC;AACnE,CAAC;AAACvB,EAAA,CAlDID,oBAAoB;AAoD1B,eAAeA,oBAAoB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}